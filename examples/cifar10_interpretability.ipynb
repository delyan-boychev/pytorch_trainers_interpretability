{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability analysis CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install build\n",
    "try:\n",
    "    import pytorch_trainers_interpretability\n",
    "except:\n",
    "    !cd .. && python -m build\n",
    "    clear_output()\n",
    "    !pip install --force --upgrade ../dist/pytorch_trainers_interpretability-0.0.1-py3-none-any.whl\n",
    "    !pip install torchmetrics[image] tqdm\n",
    "clear_output()\n",
    "try:\n",
    "    import pytorch_trainers_interpretability\n",
    "except:\n",
    "    raise Exception(\"Packages not installed! Please try again!\")\n",
    "print(\"Installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_trainers_interpretability.interpretability_eval import IntegratedGrad, ShapEval, RepVisualization\n",
    "from pytorch_trainers_interpretability.tools import show_image_column\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from textwrap import wrap\n",
    "from pytorch_trainers_interpretability.interpretability_eval.integrated_grad import IntegratedGrad\n",
    "from pytorch_trainers_interpretability.models.resnet_cifar import ResNet18_cifar\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import os\n",
    "from torchmetrics.image.fid import NoTrainInceptionV3\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading models\n",
    "It may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"./standard_cifar10.pt\"):\n",
    "    !python gdownload.py \"1--6YSHDUNcwXvnjUgjdOcV_A3bMcHEbD\" \"standard_cifar10.pt\"\n",
    "if not os.path.isfile(\"./robust_l2_cifar10.pt\"):\n",
    "    !python gdownload.py \"1OcvlvVlsC1oMIzZedvf3Zh6Q10kKzoyN\" \"robust_l2_cifar10.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=True)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=200,\n",
    "                                         shuffle=True)\n",
    "\n",
    "normalizer = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18_cifar()\n",
    "model2 = ResNet18_cifar()\n",
    "model.load_state_dict(torch.load(\"./standard_cifar10.pt\")[\"model_state_dict\"])\n",
    "model2.load_state_dict(torch.load(\"./robust_l2_cifar10.pt\")[\"model_state_dict\"])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model2.to(device)\n",
    "model.eval()\n",
    "model2.eval()\n",
    "vis = RepVisualization(model, normalizer=normalizer)\n",
    "vis2 = RepVisualization(model2, normalizer=normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Inversion\n",
    "This process in computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels =  next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.concat([images[3:4] for i in range(5)])\n",
    "r = torch.rand_like(images[0:2])/20 + 0.5\n",
    "im_n = torch.concat([images[0:3], r], 0)\n",
    "res = [im.cpu()]\n",
    "res.append(vis2.rep_inversion(im, im_n.clone()).cpu())\n",
    "res.append(vis.rep_inversion(im, im_n.clone()).cpu())\n",
    "show_image_column([\"Original image\", \"Source\", r\"Robust $l_{2}$\", r\"Robust $l_{\\lnfty}$\", \"Standard\"], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = NoTrainInceptionV3(\"inc\",features_list=[\"192\"]).cuda()\n",
    "d1 = inc(transforms.Resize((299, 299))((im*255).type(torch.uint8).cuda()))\n",
    "d2 = inc(transforms.Resize((299, 299))((res[2]*255).type(torch.uint8).cuda()))\n",
    "d3 = inc(transforms.Resize((299, 299))((res[3]*255).type(torch.uint8).cuda()))\n",
    "print(\"L2 model\")\n",
    "print(f\"L2 distnace: {round(torch.norm(d2-d1, p=2).cpu().item(), 2)}\")\n",
    "print(\"Standard model\")\n",
    "print(f\"L2 distnace: {round(torch.norm(d3-d1, p=2).cpu().item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Specific Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAIN = 4\n",
    "DATA_SHAPE = 32\n",
    "def downsample(x, step=GRAIN):\n",
    "    down = torch.zeros([len(x), 3, DATA_SHAPE//step, DATA_SHAPE//step])\n",
    "\n",
    "    for i in range(0, DATA_SHAPE, step):\n",
    "        for j in range(0, DATA_SHAPE, step):\n",
    "            v = x[:, :, i:i+step, j:j+step].mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            ii, jj = i // step, j // step\n",
    "            down[:, :, ii:ii+1, jj:jj+1] = v\n",
    "    return down\n",
    "\n",
    "def upsample(x, step=GRAIN):\n",
    "    up = torch.zeros([len(x), 3, DATA_SHAPE, DATA_SHAPE])\n",
    "\n",
    "    for i in range(0, DATA_SHAPE, step):\n",
    "        for j in range(0, DATA_SHAPE, step):\n",
    "            ii, jj = i // step, j // step\n",
    "            up[:, :, i:i+step, j:j+step] = x[:, :, ii:ii+1, jj:jj+1]\n",
    "    return up\n",
    "\n",
    "im_test, targ_test = [], []\n",
    "for _, (im, targ) in enumerate(testloader):\n",
    "    im_test.append(im)\n",
    "    targ_test.append(targ)\n",
    "im_test, targ_test = torch.cat(im_test), torch.cat(targ_test)\n",
    "im_test2, targ_test2 = [], []\n",
    "for _, (im, targ) in enumerate(trainloader):\n",
    "    im_test2.append(im)\n",
    "    targ_test2.append(targ)\n",
    "im_test2, targ_test2 = torch.cat(im_test2), torch.cat(targ_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "res2 = []\n",
    "res3 = []\n",
    "cl = np.random.choice(10, size=(3, ), replace=False)\n",
    "for j in tqdm(range(len(cl))):\n",
    "        imc = im_test[targ_test == int(cl[j])]\n",
    "        down_flat = downsample(imc).view(len(imc), -1)\n",
    "        mean = down_flat.mean(dim=0)\n",
    "        down_flat = down_flat - mean.unsqueeze(dim=0)\n",
    "        cov = down_flat.t() @ down_flat / len(imc)\n",
    "        dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov+1e-4*torch.eye(int(3 * DATA_SHAPE//GRAIN * DATA_SHAPE//GRAIN)))\n",
    "        img_seed = torch.cat([dist.sample().view(3, DATA_SHAPE//GRAIN, DATA_SHAPE//GRAIN).unsqueeze(0) for p in range(3)])\n",
    "        img_seed = upsample(torch.clamp(img_seed, min=0, max=1))\n",
    "        res3.append(img_seed)\n",
    "        res.append(vis.class_im_gen(img_seed, int(cl[j]), tqdm=False, num_iter=60, epsilon=40, lr=1).cpu())\n",
    "        res2.append(vis2.class_im_gen(img_seed, int(cl[j]), tqdm=False, num_iter=60, epsilon=40, lr=1).cpu())\n",
    "list_labels = [classes[int(cl[i])] for i in range(len(cl))]\n",
    "show_image_column(res, list_labels, fontsize=20)\n",
    "show_image_column(res2, list_labels, fontsize=20)\n",
    "show_image_column(res3, list_labels, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Score on class specific generated images\n",
    "This process in computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid1 = FrechetInceptionDistance().to(device)\n",
    "fid2 = FrechetInceptionDistance().to(device)\n",
    "fid3 = FrechetInceptionDistance().to(device)\n",
    "resize = transforms.Resize((299, 299))\n",
    "itert = tqdm(range(10))\n",
    "for i in itert:\n",
    "    imc = im_test[targ_test == i]\n",
    "    imc2 = im_test2[targ_test2 == i]\n",
    "    down_flat = downsample(imc).view(len(imc), -1)\n",
    "    mean = down_flat.mean(dim=0)\n",
    "    down_flat = down_flat - mean.unsqueeze(dim=0)\n",
    "    cov = down_flat.t() @ down_flat / len(imc)\n",
    "    dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov+1e-4*torch.eye(int(3 * DATA_SHAPE//GRAIN * DATA_SHAPE//GRAIN)))\n",
    "    for k in range(10, 60, 10):\n",
    "        imgc = imc[k-10:k]\n",
    "        img_seed = torch.cat([dist.sample().view(3, DATA_SHAPE//GRAIN, DATA_SHAPE//GRAIN).unsqueeze(0) for p in range(10)])\n",
    "        img_seed = torch.clamp(img_seed, min=0, max=1)\n",
    "        fid1.update(resize((imgc*255).type(torch.uint8)).to(device), real=True)\n",
    "        fid2.update(resize((imgc*255).type(torch.uint8)).to(device), real=True)\n",
    "        fid3.update(resize((imgc*255).type(torch.uint8)).to(device), real=True)\n",
    "        img_g = vis.class_im_gen(img_seed, i, tqdm=False, num_iter=60, epsilon=40, lr=1).cpu()\n",
    "        img_g2 = vis2.class_im_gen(img_seed, i, tqdm=False, num_iter=60, epsilon=40, lr=1).cpu()\n",
    "        fid1.update(resize((img_g*255).type(torch.uint8)).to(device), real=False)\n",
    "        fid2.update(resize((img_g2*255).type(torch.uint8)).to(device), real=False)\n",
    "        fid3.update(resize((imc2[k-10:k]*255).type(torch.uint8)).to(device), real=False)\n",
    "print(f\"Standard model: {round(fid1.compute().cpu().item(), 2)}\")\n",
    "print(f\"L2 model: {round(fid2.compute().cpu().item(), 2)}\")\n",
    "print(f\"Real data: {round(fid3.compute().cpu().item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Integrated Gradients Overlays and SHAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_grads_compare_3(standard, robust_l2, images, labels, classes, normalizer=lambda x: x):\n",
    "    integrated_grad = IntegratedGrad(standard, normalizer=normalizer)\n",
    "    integrated_grad2 = IntegratedGrad(robust_l2, normalizer=normalizer)\n",
    "    num_img = labels.shape[0]\n",
    "    labels_text = [ '\\n'.join(wrap(classes[l.item()], 20)) for l in labels.cpu() ]\n",
    "    num_cols = np.ceil(num_img/8).astype(int)\n",
    "    fig = plt.figure(figsize=(15.6*num_cols, 48))\n",
    "    subfigs = fig.subfigures(nrows=1, ncols=num_cols)\n",
    "    k = 0\n",
    "    for j, sub in enumerate(subfigs):\n",
    "        sub2 = sub.subfigures(nrows=8, ncols=1)\n",
    "        for i, subfig in enumerate(sub2):\n",
    "            if num_img == k:\n",
    "                break\n",
    "            axs = subfig.subplots(nrows=1, ncols=3)\n",
    "            if i is 0:\n",
    "                axs[0].set_title(\"Original image\", fontsize=40)\n",
    "                axs[1].set_title(\"Standard\", fontsize=40)\n",
    "                axs[2].set_title(r\"Robust $l_{2}$\", fontsize=40)\n",
    "            img = images[k:k+1].to(device)\n",
    "            img = normalizer(img)\n",
    "            pr = standard(img)\n",
    "            pr2 = robust_l2(img)\n",
    "            image = images[k].cpu().permute(1, 2, 0).numpy()\n",
    "            grad, _ = integrated_grad.random_baseline_integrated_grads(image, pr.argmax(dim=1).item(), steps=50, num_random_trials=10, batch_size=10)\n",
    "            grad2, _ = integrated_grad2.random_baseline_integrated_grads(image, pr2.argmax(dim=1).item(), steps=50, num_random_trials=10, batch_size=10)\n",
    "            pred = '\\n'.join(wrap(classes[pr.argmax(dim=1).item()], 12))\n",
    "            pred2 = '\\n'.join(wrap(classes[pr2.argmax(dim=1).item()], 12))\n",
    "            axs[0].set_xlabel('\\n'.join(wrap(labels_text[k], 10)), fontsize=35)\n",
    "            axs[0].imshow(image)\n",
    "            axs[1].set_xlabel(f\"{pred}\", fontsize=35)\n",
    "            axs[1].imshow(integrated_grad.visualization(grad, image))\n",
    "            axs[2].set_xlabel(f\"{pred2}\", fontsize=35)\n",
    "            axs[2].imshow(integrated_grad2.visualization(grad2, image))\n",
    "            for t in range(3):\n",
    "                axs[t].set_xticklabels([])\n",
    "                axs[t].set_yticklabels([])\n",
    "            k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeval = ShapEval(model, classes, normalizer)\n",
    "shapeval.gradient_exp(images[:174], images[174:200], labels[174:200])\n",
    "shapeval2 = ShapEval(model2, classes, normalizer)\n",
    "shapeval2.gradient_exp(images[:174], images[174:200], labels[174:200])\n",
    "int_grads_compare_3(model, model2, images[174:200], labels[174:200], classes, normalizer)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
